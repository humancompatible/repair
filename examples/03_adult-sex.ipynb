{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Post-Processing: Adult Income (`Sex` as Protected Attribute)\n",
    "\n",
    "In this notebook, we switch from race to **sex** as the protected attribute.  \n",
    "We will:\n",
    "\n",
    "1. **Load & preprocess data** (sensitive attribute = **sex**)  \n",
    "   - Define **groups** based on `S = 0/1`  \n",
    "\n",
    "2. **Build a Random Forest baseline**  \n",
    "\n",
    "3. **Apply post-processing methods**:  \n",
    "   - **Origin** - no repair  \n",
    "   - **Barycentre** - OT-based full repair  \n",
    "   - **Partial** - our tunable, softer repair  \n",
    "   - **ROC** - baseline post-processing; favorable outcomes are assigned to the unprivileged group within a confidence band around the decision boundary ([Kamiran et al., 2012](https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html))  \n",
    "\n",
    "4. **Compare metrics**: Disparate Impact, F1 variants, and TV distance  \n",
    "\n",
    "5. **Compute feature importance** and draw conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Imports & basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aif360.datasets import AdultDataset\n",
    "\n",
    "from humancompatible.repair.methods.data_analysis import rdata_analysis\n",
    "from humancompatible.repair.postprocess.roc_postprocess import ROCpostprocess\n",
    "from humancompatible.repair.postprocess.proj_postprocess import Projpostprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We silence FutureWarnings so youâ€™ll only see the key outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Utility helpers\n",
    "\n",
    "Here are three functions:\n",
    "\n",
    "- **`load_data`** - merges train/test, encodes `S` & `Y`, bins skewed numerics  \n",
    "\n",
    "- **`categerise`** - simple numeric binning for one column  \n",
    "\n",
    "- **`choose_x`** - computes per-feature Total-Variation to shortlist the most imbalanced axes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path,var_list,pa):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "                'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "                'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                'native-country', 'Y']\n",
    "    na_values=['?']\n",
    "    pa_dict={'Male':1,'Female':0,'White':1,'Black':0}\n",
    "    label_dict={'>50K.':1,'>50K':1,'<=50K.':0,'<=50K':0}\n",
    "    train_path = os.path.join(data_path, 'adult.data')\n",
    "    test_path = os.path.join(data_path, 'adult.test')\n",
    "    train = pd.read_csv(train_path, header=None,names=column_names,\n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "    test = pd.read_csv(test_path, header=0,names=column_names,\n",
    "                    skipinitialspace=True, na_values=na_values)\n",
    "    messydata = pd.concat([test, train], ignore_index=True)[var_list+[pa,'Y']]\n",
    "    messydata=messydata.rename(columns={pa:'S'})\n",
    "    messydata['S']=messydata['S'].replace(pa_dict)\n",
    "    messydata['Y']=messydata['Y'].replace(label_dict)\n",
    "    messydata=messydata[(messydata['S']==0)|(messydata['S']==1)]\n",
    "    for col in var_list+['S','Y']:\n",
    "        messydata[col]=messydata[col].astype('int64')\n",
    "    messydata['W']=1\n",
    "    bins_capitalgain=[100,3500,7500,10000]\n",
    "    bins_capitalloss=[100,1600,1900,2200]\n",
    "    bins_age=[26,36,46,56]\n",
    "    bins_hours=[21,36,46,61]\n",
    "\n",
    "    messydata=categerise(messydata,'age',bins_age)\n",
    "    # messydata=categerise(messydata,'hours-per-week',bins_hours)\n",
    "    messydata=categerise(messydata,'capital-gain',bins_capitalgain)\n",
    "    messydata=categerise(messydata,'capital-loss',bins_capitalloss)\n",
    "    \n",
    "    return messydata\n",
    "\n",
    "def categerise(df,col,bins):\n",
    "    for i in range(len(bins)+1):\n",
    "        if i == 0:\n",
    "            df.loc[df[col] < bins[i], col] = i\n",
    "        elif i == len(bins):\n",
    "            df.loc[df[col] >= bins[i-1], col] = i\n",
    "        else:\n",
    "            df.loc[(df[col] >= bins[i-1])& (df[col] < bins[i]), col] = i        \n",
    "    return df\n",
    "\n",
    "def choose_x(var_list,messydata):\n",
    "    tv_dist=dict()\n",
    "    for x_name in var_list:\n",
    "        x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "        dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "        tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "    x_list=[]\n",
    "    for key,val in tv_dist.items():\n",
    "        if val>0.1:\n",
    "            x_list+=[key]  \n",
    "    return x_list,tv_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protected-attribute setup\n",
    "\n",
    "- **Protected attribute**: `pa = \"sex\"`\n",
    "\n",
    "- **Privileged** = Male (`S = 1`), **Unprivileged** = Female (`S = 0`)  \n",
    "\n",
    "- We tune our threshold grid around 0.05, since sex-based gaps tend to be subtler than race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='..//data//adult'\n",
    "var_list=['age','capital-gain','capital-loss','education-num'] #'hours-per-week',\n",
    "pa='sex'\n",
    "favorable_label = 1\n",
    "var_dim=len(var_list)\n",
    "\n",
    "K=200\n",
    "e=0.01\n",
    "\n",
    "if pa == 'sex':\n",
    "    thresh=0.05\n",
    "elif pa == 'race':\n",
    "    thresh=0.1\n",
    "\n",
    "messydata = load_data(data_path,var_list,pa)\n",
    "x_list,tv_dist = choose_x(var_list,messydata)\n",
    "\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': np.float64(0.1010227688866829),\n",
       " 'capital-gain': np.float64(0.036924675713792855),\n",
       " 'capital-loss': np.float64(0.020068855964263464),\n",
       " 'education-num': np.float64(0.07095473385227195)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Training & post-processing experiment\n",
    "\n",
    "For each of 10 random splits we:\n",
    "\n",
    "1. Fit a depth-5 Random-Forest\n",
    "\n",
    "2. Run **Origin**, **Barycentre**, and **Partial** repairs\n",
    "\n",
    "3. (Skip ROC here for brevity - see `02_adult.ipynb` for ROC details)\n",
    "\n",
    "4. Record DI, F1 (macro/micro/weighted), and TV distance on our two repaired axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=0.05\n",
    "x_list = ['age','education-num']\n",
    "methods=['origin','barycentre','partial']\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(10):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "    \n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label,linspace_range=(0.1,0.9),theta=1e-3)\n",
    "    for method in methods[:-1]:\n",
    "        report = pd.concat([report,projpost.postprocess(method)], ignore_index=True)\n",
    "    \n",
    "    for p in [1e-2,1e-3,1e-4]:\n",
    "        report = pd.concat([report,projpost.postprocess('partial',para=p)], ignore_index=True)\n",
    "\n",
    "report.to_csv('../data/E3_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate across folds. In the table below:\n",
    "\n",
    "- **DI**: Disparate Impact\n",
    "\n",
    "- **F1â€†(macro/micro/weighted)**: classification quality\n",
    "\n",
    "- **TV distance**: remaining gap on the repaired features\n",
    "\n",
    "- **method**: repair strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DI</th>\n",
       "      <th>f1 macro</th>\n",
       "      <th>f1 micro</th>\n",
       "      <th>f1 weighted</th>\n",
       "      <th>TV distance</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453587</td>\n",
       "      <td>0.649873</td>\n",
       "      <td>0.810309</td>\n",
       "      <td>0.77292</td>\n",
       "      <td>0.128614</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.466078</td>\n",
       "      <td>0.646376</td>\n",
       "      <td>0.805293</td>\n",
       "      <td>0.769449</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.104568</td>\n",
       "      <td>0.581083</td>\n",
       "      <td>0.722219</td>\n",
       "      <td>0.707322</td>\n",
       "      <td>0.086938</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.827115</td>\n",
       "      <td>0.634161</td>\n",
       "      <td>0.763577</td>\n",
       "      <td>0.747127</td>\n",
       "      <td>0.027393</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.822652</td>\n",
       "      <td>0.635626</td>\n",
       "      <td>0.764856</td>\n",
       "      <td>0.748285</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.553847</td>\n",
       "      <td>0.675905</td>\n",
       "      <td>0.814608</td>\n",
       "      <td>0.786284</td>\n",
       "      <td>0.134076</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.535066</td>\n",
       "      <td>0.664566</td>\n",
       "      <td>0.804218</td>\n",
       "      <td>0.777242</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.094631</td>\n",
       "      <td>0.592422</td>\n",
       "      <td>0.724011</td>\n",
       "      <td>0.712987</td>\n",
       "      <td>0.092245</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.149719</td>\n",
       "      <td>0.57215</td>\n",
       "      <td>0.690792</td>\n",
       "      <td>0.689443</td>\n",
       "      <td>0.02805</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.086683</td>\n",
       "      <td>0.591588</td>\n",
       "      <td>0.683933</td>\n",
       "      <td>0.69269</td>\n",
       "      <td>0.00374</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.476181</td>\n",
       "      <td>0.674329</td>\n",
       "      <td>0.818191</td>\n",
       "      <td>0.787014</td>\n",
       "      <td>0.13021</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.497095</td>\n",
       "      <td>0.674527</td>\n",
       "      <td>0.817219</td>\n",
       "      <td>0.786719</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.099291</td>\n",
       "      <td>0.601784</td>\n",
       "      <td>0.731023</td>\n",
       "      <td>0.719888</td>\n",
       "      <td>0.086447</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.136441</td>\n",
       "      <td>0.588758</td>\n",
       "      <td>0.703127</td>\n",
       "      <td>0.701662</td>\n",
       "      <td>0.02681</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.062356</td>\n",
       "      <td>0.600221</td>\n",
       "      <td>0.717971</td>\n",
       "      <td>0.713174</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.475506</td>\n",
       "      <td>0.670696</td>\n",
       "      <td>0.815939</td>\n",
       "      <td>0.784282</td>\n",
       "      <td>0.133848</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.517697</td>\n",
       "      <td>0.669485</td>\n",
       "      <td>0.814096</td>\n",
       "      <td>0.783032</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.057394</td>\n",
       "      <td>0.598846</td>\n",
       "      <td>0.729436</td>\n",
       "      <td>0.717721</td>\n",
       "      <td>0.093361</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.037755</td>\n",
       "      <td>0.594039</td>\n",
       "      <td>0.719916</td>\n",
       "      <td>0.711446</td>\n",
       "      <td>0.026831</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.050734</td>\n",
       "      <td>0.583925</td>\n",
       "      <td>0.691867</td>\n",
       "      <td>0.693993</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.442412</td>\n",
       "      <td>0.648099</td>\n",
       "      <td>0.814608</td>\n",
       "      <td>0.774985</td>\n",
       "      <td>0.128793</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.474124</td>\n",
       "      <td>0.649988</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>0.775461</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.119799</td>\n",
       "      <td>0.571961</td>\n",
       "      <td>0.728054</td>\n",
       "      <td>0.707455</td>\n",
       "      <td>0.082893</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.156227</td>\n",
       "      <td>0.578466</td>\n",
       "      <td>0.702718</td>\n",
       "      <td>0.698431</td>\n",
       "      <td>0.026306</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.890518</td>\n",
       "      <td>0.613947</td>\n",
       "      <td>0.756309</td>\n",
       "      <td>0.736833</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.446163</td>\n",
       "      <td>0.658551</td>\n",
       "      <td>0.811435</td>\n",
       "      <td>0.776211</td>\n",
       "      <td>0.123399</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.43347</td>\n",
       "      <td>0.653871</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>0.772659</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.074383</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.726058</td>\n",
       "      <td>0.715218</td>\n",
       "      <td>0.085638</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.057329</td>\n",
       "      <td>0.589779</td>\n",
       "      <td>0.698828</td>\n",
       "      <td>0.698698</td>\n",
       "      <td>0.026963</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.762128</td>\n",
       "      <td>0.655781</td>\n",
       "      <td>0.784563</td>\n",
       "      <td>0.764205</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.596482</td>\n",
       "      <td>0.680048</td>\n",
       "      <td>0.815376</td>\n",
       "      <td>0.789058</td>\n",
       "      <td>0.124823</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.549424</td>\n",
       "      <td>0.674072</td>\n",
       "      <td>0.807493</td>\n",
       "      <td>0.783317</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.013805</td>\n",
       "      <td>0.613324</td>\n",
       "      <td>0.746788</td>\n",
       "      <td>0.732335</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.125278</td>\n",
       "      <td>0.583881</td>\n",
       "      <td>0.700619</td>\n",
       "      <td>0.699345</td>\n",
       "      <td>0.027454</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.07046</td>\n",
       "      <td>0.601911</td>\n",
       "      <td>0.696832</td>\n",
       "      <td>0.703747</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.614358</td>\n",
       "      <td>0.678284</td>\n",
       "      <td>0.82029</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.124002</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.640838</td>\n",
       "      <td>0.677877</td>\n",
       "      <td>0.818293</td>\n",
       "      <td>0.791144</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.04601</td>\n",
       "      <td>0.609316</td>\n",
       "      <td>0.751497</td>\n",
       "      <td>0.734837</td>\n",
       "      <td>0.088879</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.108835</td>\n",
       "      <td>0.587263</td>\n",
       "      <td>0.680606</td>\n",
       "      <td>0.691798</td>\n",
       "      <td>0.027232</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.901325</td>\n",
       "      <td>0.653753</td>\n",
       "      <td>0.757281</td>\n",
       "      <td>0.754587</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.555918</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>0.814659</td>\n",
       "      <td>0.78583</td>\n",
       "      <td>0.131178</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.498398</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>0.804064</td>\n",
       "      <td>0.779564</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.124381</td>\n",
       "      <td>0.588437</td>\n",
       "      <td>0.726314</td>\n",
       "      <td>0.71267</td>\n",
       "      <td>0.089437</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.11132</td>\n",
       "      <td>0.593365</td>\n",
       "      <td>0.696371</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.099888</td>\n",
       "      <td>0.590391</td>\n",
       "      <td>0.687874</td>\n",
       "      <td>0.694604</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.407606</td>\n",
       "      <td>0.64645</td>\n",
       "      <td>0.812919</td>\n",
       "      <td>0.772426</td>\n",
       "      <td>0.130432</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.408636</td>\n",
       "      <td>0.645579</td>\n",
       "      <td>0.810769</td>\n",
       "      <td>0.771224</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.583492</td>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.718384</td>\n",
       "      <td>0.081377</td>\n",
       "      <td>partial_0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.039093</td>\n",
       "      <td>0.583423</td>\n",
       "      <td>0.727082</td>\n",
       "      <td>0.710453</td>\n",
       "      <td>0.025223</td>\n",
       "      <td>partial_0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.039553</td>\n",
       "      <td>0.587185</td>\n",
       "      <td>0.726314</td>\n",
       "      <td>0.711631</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>partial_0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DI  f1 macro  f1 micro f1 weighted TV distance          method\n",
       "0   0.453587  0.649873  0.810309     0.77292    0.128614          origin\n",
       "1   0.466078  0.646376  0.805293    0.769449    0.000627      barycentre\n",
       "2   1.104568  0.581083  0.722219    0.707322    0.086938    partial_0.01\n",
       "3   0.827115  0.634161  0.763577    0.747127    0.027393   partial_0.001\n",
       "4   0.822652  0.635626  0.764856    0.748285    0.003617  partial_0.0001\n",
       "5   0.553847  0.675905  0.814608    0.786284    0.134076          origin\n",
       "6   0.535066  0.664566  0.804218    0.777242    0.000473      barycentre\n",
       "7   1.094631  0.592422  0.724011    0.712987    0.092245    partial_0.01\n",
       "8   1.149719   0.57215  0.690792    0.689443     0.02805   partial_0.001\n",
       "9   1.086683  0.591588  0.683933     0.69269     0.00374  partial_0.0001\n",
       "10  0.476181  0.674329  0.818191    0.787014     0.13021          origin\n",
       "11  0.497095  0.674527  0.817219    0.786719    0.001386      barycentre\n",
       "12  1.099291  0.601784  0.731023    0.719888    0.086447    partial_0.01\n",
       "13  1.136441  0.588758  0.703127    0.701662     0.02681   partial_0.001\n",
       "14  1.062356  0.600221  0.717971    0.713174    0.003734  partial_0.0001\n",
       "15  0.475506  0.670696  0.815939    0.784282    0.133848          origin\n",
       "16  0.517697  0.669485  0.814096    0.783032     0.00005      barycentre\n",
       "17  1.057394  0.598846  0.729436    0.717721    0.093361    partial_0.01\n",
       "18  1.037755  0.594039  0.719916    0.711446    0.026831   partial_0.001\n",
       "19  1.050734  0.583925  0.691867    0.693993    0.003631  partial_0.0001\n",
       "20  0.442412  0.648099  0.814608    0.774985    0.128793          origin\n",
       "21  0.474124  0.649988  0.813687    0.775461    0.000701      barycentre\n",
       "22  1.119799  0.571961  0.728054    0.707455    0.082893    partial_0.01\n",
       "23  1.156227  0.578466  0.702718    0.698431    0.026306   partial_0.001\n",
       "24  0.890518  0.613947  0.756309    0.736833    0.004276  partial_0.0001\n",
       "25  0.446163  0.658551  0.811435    0.776211    0.123399          origin\n",
       "26   0.43347  0.653871  0.807596    0.772659    0.000408      barycentre\n",
       "27  1.074383  0.598943  0.726058    0.715218    0.085638    partial_0.01\n",
       "28  1.057329  0.589779  0.698828    0.698698    0.026963   partial_0.001\n",
       "29  0.762128  0.655781  0.784563    0.764205    0.003996  partial_0.0001\n",
       "30  0.596482  0.680048  0.815376    0.789058    0.124823          origin\n",
       "31  0.549424  0.674072  0.807493    0.783317    0.000824      barycentre\n",
       "32  1.013805  0.613324  0.746788    0.732335    0.082698    partial_0.01\n",
       "33  1.125278  0.583881  0.700619    0.699345    0.027454   partial_0.001\n",
       "34   1.07046  0.601911  0.696832    0.703747    0.003944  partial_0.0001\n",
       "35  0.614358  0.678284   0.82029    0.792118    0.124002          origin\n",
       "36  0.640838  0.677877  0.818293    0.791144     0.00023      barycentre\n",
       "37   1.04601  0.609316  0.751497    0.734837    0.088879    partial_0.01\n",
       "38  1.108835  0.587263  0.680606    0.691798    0.027232   partial_0.001\n",
       "39  0.901325  0.653753  0.757281    0.754587    0.003717  partial_0.0001\n",
       "40  0.555918  0.674352  0.814659     0.78583    0.131178          origin\n",
       "41  0.498398  0.669692  0.804064    0.779564    0.001055      barycentre\n",
       "42  1.124381  0.588437  0.726314     0.71267    0.089437    partial_0.01\n",
       "43   1.11132  0.593365  0.696371      0.7001    0.028804   partial_0.001\n",
       "44  1.099888  0.590391  0.687874    0.694604    0.003829  partial_0.0001\n",
       "45  0.407606   0.64645  0.812919    0.772426    0.130432          origin\n",
       "46  0.408636  0.645579  0.810769    0.771224    0.000568      barycentre\n",
       "47  0.999035  0.583492  0.745509    0.718384    0.081377    partial_0.01\n",
       "48  1.039093  0.583423  0.727082    0.710453    0.025223   partial_0.001\n",
       "49  1.039553  0.587185  0.726314    0.711631    0.004436  partial_0.0001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.544755  0.677207  0.814403    0.786268     0.12403  origin\n",
      "        DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.56826  0.674245  0.813789    0.784739    0.000754  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.594879  0.660944  0.809131     0.77711    0.083422  partial_0.01\n",
      "         DI f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.592138  0.66598  0.810411    0.779809    0.025305  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.562569  0.681667  0.817116    0.789727    0.130114  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.592823  0.674918  0.815069    0.785996    0.000073  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.605047  0.662321  0.811281    0.779035    0.090797  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.608203  0.662164  0.810872    0.778806    0.027678  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.444107  0.654685  0.811793    0.775347    0.131739  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.473131  0.648631  0.810513    0.772182    0.000526  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.469953  0.632754  0.806214    0.763504    0.091235  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.474958  0.632322  0.805753    0.763139    0.028561  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.558261  0.680204  0.816041    0.789179    0.140592  origin\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.575481  0.677977  0.815837    0.788142     0.00045  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.587111  0.664713  0.811639    0.780761    0.095662  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.596032  0.660723  0.810411    0.778552     0.03066  partial_0.001\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance  method\n",
      "0  0.572249  0.672913  0.816246    0.788273    0.129616  origin\n",
      "         DI f1 macro  f1 micro f1 weighted TV distance      method\n",
      "0  0.584778  0.67065  0.815734    0.787113     0.00057  barycentre\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance        method\n",
      "0  0.606497  0.653858  0.811128    0.778167    0.091507  partial_0.01\n",
      "         DI  f1 macro  f1 micro f1 weighted TV distance         method\n",
      "0  0.617182  0.654007  0.810565    0.778007    0.028526  partial_0.001\n"
     ]
    }
   ],
   "source": [
    "thresh=0.3\n",
    "x_list = ['age','education-num']\n",
    "methods=['origin','barycentre','partial'] # Place ROC in the end\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(5):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "\n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label,linspace_range=(0.1,0.9),theta=1e-3)\n",
    "    \n",
    "    print(projpost.postprocess('origin'))\n",
    "    print(projpost.postprocess('barycentre'))\n",
    "    for t in range(2,4):\n",
    "        print(projpost.postprocess('partial',para=10**(-t)))\n",
    "\n",
    "# report.to_csv('../data/E3_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional threshold =  [0.1        0.18888889 0.27777778 0.36666667 0.45555556 0.54444444\n",
      " 0.63333333 0.72222222 0.81111111 0.9       ]\n",
      "Disparate Impact =  [0.70823508 0.65742979 0.6418957  0.63642532 0.63418972 0.62317166\n",
      " 0.63852072 0.63852072 0.6484239  0.69129834]\n",
      "f1 scores =  [0.69063882 0.66558246 0.66209947 0.66202684 0.6622522  0.6616947\n",
      " 0.65925716 0.65925716 0.65472653 0.64197352]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valpost = Projpostprocess(X_val,y_val,x_list,var_list,clf,K,e,'auto',linspace_range=(0.1,0.9),theta=1e-3)\n",
    "valpost.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing results\n",
    "\n",
    "- **Origin** buys the highest F1 but sits far below fairness (DI â‰ˆ 0.50).\n",
    "\n",
    "- **Barycentre** nearly closes the gap (TV â‰ˆ 0) at only a tiny F1 penalty.\n",
    "\n",
    "- **Partial** gives you a \"fairness knob\" - small `t` -> light fix, large `t` -> full parity at more cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0189\n",
      "Optimal classification threshold (with fairness constraints) = 0.2700\n",
      "Optimal ROC margin = 0.0300\n",
      "Optimal classification threshold (with fairness constraints) = 0.1700\n",
      "Optimal ROC margin = 0.0189\n",
      "Optimal classification threshold (with fairness constraints) = 0.3100\n",
      "Optimal ROC margin = 0.0344\n",
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n",
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n",
      "Optimal classification threshold (with fairness constraints) = 0.1500\n",
      "Optimal ROC margin = 0.0167\n",
      "Optimal classification threshold (with fairness constraints) = 0.1900\n",
      "Optimal ROC margin = 0.0211\n",
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n",
      "Optimal classification threshold (with fairness constraints) = 0.2900\n",
      "Optimal ROC margin = 0.0322\n"
     ]
    }
   ],
   "source": [
    "methods=['origin','unconstrained','barycentre','partial','ROC'] # Place ROC in the end\n",
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(10):\n",
    "    # train val test 4:2:4\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "\n",
    "    clf=RandomForestClassifier(max_depth=5).fit(X_train[:,0:var_dim],y_train)\n",
    "    projpost = Projpostprocess(X_test,y_test,x_list,var_list,clf,K,e,thresh,favorable_label,linspace_range=(0.1,0.9),theta=1e-3)\n",
    "    for method in methods[:-1]:\n",
    "        # report = pd.concat([report,projpost.postprocess(method,para=1e-2)], ignore_index=True)\n",
    "        report = pd.concat([report,projpost.postprocess(method,para=1e-3)], ignore_index=True)\n",
    "\n",
    "    ROCpost = ROCpostprocess(X_val,y_val,var_list,clf,favorable_label) # use validation set to train a ROC model\n",
    "    report = pd.concat([report,ROCpost.postprocess(X_test,y_test,tv_origin=projpost.tv_origin)], ignore_index=True)\n",
    "\n",
    "report.to_csv('../data/report_postprocess_adult_'+str(pa)+'.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Compute average feature importance\n",
    "\n",
    "Finally, we revisit our Random-Forest baseline to see which inputs drive income prediction most:\n",
    "\n",
    "* **capital-gain** and **education-num** lead the pack  \n",
    "\n",
    "* **age** follows closely  \n",
    "\n",
    "These are the features youâ€™d want to guard most carefully against proxying sensitive traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': np.float64(0.04744857663969924),\n",
       " 'education-num': np.float64(0.056762405582129784),\n",
       " 'capital-gain': np.float64(0.021127950774052707),\n",
       " 'capital-loss': np.float64(0.011363681253224836),\n",
       " 'hours-per-week': np.float64(0.04445283428803036)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa = 'race'\n",
    "label_map = {1.0: '>50K', 0.0: '<=50K'}\n",
    "privileged_groups = [{pa: 1}]\n",
    "unprivileged_groups = [{pa: 0}]\n",
    "if pa == 'sex':\n",
    "    thresh=0.05\n",
    "    protected_attribute_maps = [{1.0: 'Male', 0.0: 'Female'}]\n",
    "    cd = AdultDataset(protected_attribute_names=[pa],privileged_classes=[['Male'],[1.0]], \n",
    "        metadata={'label_map': label_map,'protected_attribute_maps': protected_attribute_maps},\n",
    "        # categorical_features=['workclass', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
    "        features_to_drop=['race','fnlwgt','education','relationship',\n",
    "                          'native-country','workclass','marital-status','occupation'])\n",
    "elif pa == 'race':\n",
    "    thresh=0.1\n",
    "    protected_attribute_maps = [{1.0: 'White', 0.0:'Non-white'}]\n",
    "    cd = AdultDataset(protected_attribute_names=[pa],privileged_classes=[['White'],[1.0]],\n",
    "        metadata={'label_map': label_map,'protected_attribute_maps': protected_attribute_maps}, #\n",
    "        features_to_drop=['sex','fnlwgt','education','relationship',\n",
    "                          'native-country','workclass','marital-status','occupation'])\n",
    "    #,'workclass','marital-status','occupation','relationship',\n",
    "\n",
    "# train,test = cd.split([0.6], shuffle=True) #len(test.instance_names) = 2057\n",
    "var_list = cd.feature_names.copy()\n",
    "var_list.remove(pa)\n",
    "var_dim=len(var_list)\n",
    "\n",
    "K=200\n",
    "e=0.01\n",
    "bins_capitalgain=[100,3500,7500,10000]\n",
    "bins_capitalloss=[100,1600,1900,2200]\n",
    "\n",
    "messydata=cd.convert_to_dataframe()[0]\n",
    "messydata=messydata.rename(columns={pa:'S',cd.label_names[0]:'Y'})\n",
    "messydata=messydata[(messydata['S']==1)|(messydata['S']==0)]\n",
    "for col in var_list+['S','Y']:\n",
    "    messydata[col]=messydata[col].astype('int64')\n",
    "messydata['W']=cd.instance_weights\n",
    "# project 0-100 to {0,1,...,5}\n",
    "messydata['age']=np.floor((messydata['age'].to_numpy()-17)/15)\n",
    "messydata['hours-per-week']=np.floor(messydata['hours-per-week'].to_numpy()/20)\n",
    "messydata=categerise(messydata,'capital-gain',bins_capitalgain)\n",
    "messydata=categerise(messydata,'capital-loss',bins_capitalloss)\n",
    "\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "x_list=[]\n",
    "for key,val in tv_dist.items():\n",
    "    if val>0.045:\n",
    "        x_list+=[key]        \n",
    "tv_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Conclusions\n",
    "\n",
    "In this notebook we compared several simple, model-agnostic post-processing strategies on a Random-Forest income predictor. \n",
    "Hereâ€™s what we found:\n",
    "\n",
    "- **Post-processing is practical**: you can retrofit fairness without re-training.\n",
    "\n",
    "- **Barycentre is powerful**: it erases group gaps with minimal accuracy loss.\n",
    "\n",
    "- **Partial repair offers flexibility**: tune `t` to hit your compliance bar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
